# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
This dataset contains data related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls.  The classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y). (Moro et al., 2014)

You can access the dataset through the following link:
https://archive.ics.uci.edu/ml/datasets/bank+marketing

It was ran two solutions, one consisting in a LogisticRegression model with scikit-learn using Azure HyperDrive and the other using Azure AutoML to find the best accuracy. 
The best perfomance model was the Azure AutoML run by millesimal differences. The accuracy obtained by the Azure AutoML was 0.91715 while by the Azure HyperDrive was 0.90872. The difference is 0.00843, a very small number. The best performing model was the VotingEnsemble. A voting ensemble works by combining the predictions from multiple models. It uses Soft Voting that predicts the class with the largest summed probability from models.

## Scikit-learn Pipeline

You can see the pipeline main steps that we followed in both approaches:

![diagram](./images/creating-and-optimizing-an-ml-pipeline.png)

### Data preparation
The dataset in a csv file was converted to a TabularDataset type by using TabularDatasetFactory and then to a pandas dataframe. The dataframe was cleaned and tranformed in One hot encoding. It was used the train_test_split method from scikit-learning to split 75% of data to the train set and 25% to the test set.

### Classifier
Logistic Regression from the Scikit-Learn library was chosen as algorithm to be tuned by the Azure HyperDrive.

### Hyperparameter tuning 
The parateters tuned was the following 
* C - Inverse of regularization strength
* max_iter - Maximum number of iterations taken for the solvers to converge
The parameters search space was:

C: (0.01, 0.1, 1, 10, 100)

max_iter (25, 50, 75, 100, 125, 150, 175, 200, 250, 300)

It was used the RandomParamaterSampler that supports both continuos and discrete values. Besides, it supports early termination of low-performance runs.
it picks randomly hyperparameter values, so in this case save some time getting and optimal primary metric in a short period.

The early termination policy chosen was Bandit Policy. It is an early termination policy based on slack factor/slack amount and evaluation interval. The policy early terminates any runs where the primary metric is not within the specified slack factor/slack amount with respect to the best performing training run. So, it avoids unnecessary processing.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
The AutoConfig configuration in the same Notebook followed these steps:

Created a dataset from the provided URL using TabularDatasetFactory in the notebook
Splited data into train and test sets
Modified the AutoML config
Submited the AutoML run
Saved the best model

 The best performing model was the VotingEnsemble. A voting ensemble works by combining the predictions from multiple models. It uses Soft Voting that predicts the class with the largest summed probability from models.


## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
